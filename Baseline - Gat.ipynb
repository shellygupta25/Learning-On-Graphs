{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ea547e",
   "metadata": {},
   "source": [
    "# Final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73035100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import time \n",
    "import torch\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATv2Conv,GATConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, accuracy_score,f1_score,roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32afb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GAT run this else run next block\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels,heads = heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(\n",
    "            dim=-1\n",
    "        )  # product of a pair of nodes on each edge\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "    \n",
    "\n",
    "def train_link_predictor(\n",
    "    model, train_data, val_data, optimizer, criterion, n_epochs=100\n",
    "):\n",
    "    best_model = model\n",
    "    best_val_auc = 0\n",
    "    val_auc_list = []\n",
    "    train_loss_list = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "        # sampling training negatives for every training epoch\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "            num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        edge_label_index = torch.cat(\n",
    "            [train_data.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        edge_label = torch.cat([\n",
    "            train_data.edge_label,\n",
    "            train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_auc = eval_link_predictor(model, val_data)\n",
    "        val_auc_list.append(val_auc)\n",
    "        if (val_auc > best_val_auc):\n",
    "            \n",
    "            torch.save(model,\"best_link_prediction.pt\")\n",
    "            best_model = model\n",
    "            best_val_auc = val_auc\n",
    "            line = \"Epoch: \"+str(epoch) + \"\\tTrain Loss: \"+str(loss) + \"\\tVal AUC: \"+str(best_val_auc)\n",
    "            \n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val AUC: {val_auc:.3f}\")\n",
    "        #train_loss_list.append({loss:.3f})\n",
    "        train_loss_list.append(loss)\n",
    "    print(\"\\n\\n for best model :\\n\",line)\n",
    "    \n",
    "    return best_model,val_auc_list,train_loss_list #,train_auc_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, data):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bef6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user set params\n",
    "dir_path = \"WSJ split data files\"\n",
    "dbname = \"WSJ\"\n",
    "node_feature_dir_path = \"WSJ node features\"\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138261a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no  1  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'WSJ split data files/WSJ_positive_edges_split_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1896692/2179225390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest_file_no\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/WSJ_positive_edges_split_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WSJ split data files/WSJ_positive_edges_split_2.csv'"
     ]
    }
   ],
   "source": [
    "for split_no in range(1,2,1):\n",
    "    test_file_no = split_no\n",
    "    print(\"Test no \", test_file_no, \" going on\")\n",
    "    print(\"\\n\\n\\n Test file no : \",test_file_no)\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in range(1,11,1):\n",
    "        if i != test_file_no:\n",
    "            df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\") \n",
    "            train_df = train_df.append(df, ignore_index=True)\n",
    "            del df\n",
    "        \n",
    "            \n",
    "    train_df = train_df.drop(['layer', 'weight',\"sign\"], axis=1)\n",
    "    \n",
    "    #making node_id variables continuous\n",
    "    temp_node_no = pickle.load(open(\"node_id_into_continuous_node_ids.p\",\"rb\"))\n",
    "    \n",
    "    # collecting all positive edges\n",
    "    train_df['src'] = [temp_node_no[i] for i in train_df['src']]\n",
    "    train_df['dest'] = [temp_node_no[i] for i in train_df['dest']]\n",
    "    final_edge_list = [torch.Tensor([train_df['src']]),torch.Tensor([train_df['dest']])]\n",
    "    final_edge_list = torch.cat(tuple(final_edge_list),dim=0)  \n",
    "    \n",
    "    # downloading  node_embeddings \n",
    "    node_embeddings = pickle.load(open(node_feature_dir_path + \"/node_embeddings_for_pytorch_models_excluding_split_\"+str(test_file_no)+\".p\",\"rb\"))\n",
    "    \n",
    "    #making training and val graphs\n",
    "    data = Data()\n",
    "    data.x = node_embeddings\n",
    "    data.edge_index = final_edge_list.type(torch.int64)\n",
    "\n",
    "    split = T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.0,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=False,\n",
    "        neg_sampling_ratio=1.0)\n",
    "    train_data, val_data, test_data = split(data)\n",
    "    \n",
    "    del train_df, node_embeddings, final_edge_list, data\n",
    "    \n",
    "    print(train_data)\n",
    "    print(val_data)\n",
    "    print(test_data)\n",
    "    break\n",
    "    model = Net(8, 3, 2) #.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    curr = time.time()\n",
    "    best_model,_,_ = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=epochs)\n",
    "    timetaken = time.time()-curr\n",
    "    print(\"time taken for training : \",timetaken)\n",
    "    print(\"time taken for per epoch : \",timetaken/epochs)\n",
    "    \n",
    "    torch.save(best_model.state_dict(), \"my_\"+str(dbname)+\"_GAT_model_state_dict_\"+str(test_file_no))\n",
    "    torch.save(best_model, \"my_\"+str(dbname)+\"_GAT_whole_model_\"+str(test_file_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f3794",
   "metadata": {},
   "source": [
    "# GATv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e26788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GAT run this else run next block\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels,heads = heads)\n",
    "        self.conv2 = GATv2Conv(hidden_channels*heads, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(\n",
    "            dim=-1\n",
    "        )  # product of a pair of nodes on each edge\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "    \n",
    "\n",
    "def train_link_predictor(\n",
    "    model, train_data, val_data, optimizer, criterion, n_epochs=100\n",
    "):\n",
    "    best_model = model\n",
    "    best_val_auc = 0\n",
    "    val_auc_list = []\n",
    "    train_loss_list = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "        # sampling training negatives for every training epoch\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "            num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        edge_label_index = torch.cat(\n",
    "            [train_data.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        edge_label = torch.cat([\n",
    "            train_data.edge_label,\n",
    "            train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_auc = eval_link_predictor(model, val_data)\n",
    "        val_auc_list.append(val_auc)\n",
    "        if (val_auc > best_val_auc):\n",
    "            \n",
    "            torch.save(model,\"best_link_prediction.pt\")\n",
    "            best_model = model\n",
    "            best_val_auc = val_auc\n",
    "            line = \"Epoch: \"+str(epoch) + \"\\tTrain Loss: \"+str(loss) + \"\\tVal AUC: \"+str(best_val_auc)\n",
    "            \n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val AUC: {val_auc:.3f}\")\n",
    "        #train_loss_list.append({loss:.3f})\n",
    "        train_loss_list.append(loss)\n",
    "    print(\"\\n\\n for best model :\\n\",line)\n",
    "    \n",
    "    return best_model,val_auc_list,train_loss_list #,train_auc_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, data):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0490dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no  1  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  1\n",
      "Epoch: 010, Train Loss: 0.672, Val AUC: 0.845\n",
      "Epoch: 020, Train Loss: 0.645, Val AUC: 0.817\n",
      "Epoch: 030, Train Loss: 0.620, Val AUC: 0.837\n",
      "Epoch: 040, Train Loss: 0.597, Val AUC: 0.836\n",
      "Epoch: 050, Train Loss: 0.582, Val AUC: 0.833\n",
      "Epoch: 060, Train Loss: 0.572, Val AUC: 0.830\n",
      "Epoch: 070, Train Loss: 0.565, Val AUC: 0.833\n",
      "Epoch: 080, Train Loss: 0.560, Val AUC: 0.835\n",
      "Epoch: 090, Train Loss: 0.556, Val AUC: 0.836\n",
      "Epoch: 100, Train Loss: 0.554, Val AUC: 0.836\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 11\tTrain Loss: tensor(0.6695, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8463994362303524\n",
      "time taken for training :  191.62732458114624\n",
      "time taken for per epoch :  1.9162732458114624\n",
      "Test no  2  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  2\n",
      "Epoch: 010, Train Loss: 0.981, Val AUC: 0.803\n",
      "Epoch: 020, Train Loss: 1.035, Val AUC: 0.617\n",
      "Epoch: 030, Train Loss: 0.835, Val AUC: 0.714\n",
      "Epoch: 040, Train Loss: 0.734, Val AUC: 0.810\n",
      "Epoch: 050, Train Loss: 0.711, Val AUC: 0.811\n",
      "Epoch: 060, Train Loss: 0.681, Val AUC: 0.792\n",
      "Epoch: 070, Train Loss: 0.665, Val AUC: 0.796\n",
      "Epoch: 080, Train Loss: 0.650, Val AUC: 0.810\n",
      "Epoch: 090, Train Loss: 0.639, Val AUC: 0.811\n",
      "Epoch: 100, Train Loss: 0.628, Val AUC: 0.811\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 46\tTrain Loss: tensor(0.7333, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8131412709509105\n",
      "time taken for training :  206.334002494812\n",
      "time taken for per epoch :  2.06334002494812\n",
      "Test no  3  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  3\n",
      "Epoch: 010, Train Loss: 0.775, Val AUC: 0.304\n",
      "Epoch: 020, Train Loss: 0.714, Val AUC: 0.551\n",
      "Epoch: 030, Train Loss: 0.671, Val AUC: 0.770\n",
      "Epoch: 040, Train Loss: 0.646, Val AUC: 0.892\n",
      "Epoch: 050, Train Loss: 0.627, Val AUC: 0.873\n",
      "Epoch: 060, Train Loss: 0.608, Val AUC: 0.862\n",
      "Epoch: 070, Train Loss: 0.590, Val AUC: 0.861\n",
      "Epoch: 080, Train Loss: 0.577, Val AUC: 0.862\n",
      "Epoch: 090, Train Loss: 0.567, Val AUC: 0.862\n",
      "Epoch: 100, Train Loss: 0.559, Val AUC: 0.861\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 37\tTrain Loss: tensor(0.6521, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8975971931557768\n",
      "time taken for training :  193.10281991958618\n",
      "time taken for per epoch :  1.9310281991958618\n",
      "Test no  4  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  4\n",
      "Epoch: 010, Train Loss: 37.469, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 18.788, Val AUC: 0.500\n",
      "Epoch: 030, Train Loss: 8.211, Val AUC: 0.726\n",
      "Epoch: 040, Train Loss: 3.234, Val AUC: 0.772\n",
      "Epoch: 050, Train Loss: 1.374, Val AUC: 0.844\n",
      "Epoch: 060, Train Loss: 0.890, Val AUC: 0.837\n",
      "Epoch: 070, Train Loss: 0.783, Val AUC: 0.789\n",
      "Epoch: 080, Train Loss: 0.734, Val AUC: 0.791\n",
      "Epoch: 090, Train Loss: 0.700, Val AUC: 0.819\n",
      "Epoch: 100, Train Loss: 0.680, Val AUC: 0.841\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 54\tTrain Loss: tensor(1.0895, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8556730137276811\n",
      "time taken for training :  191.58486366271973\n",
      "time taken for per epoch :  1.9158486366271972\n",
      "Test no  5  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  5\n",
      "Epoch: 010, Train Loss: 408.115, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 343.490, Val AUC: 0.500\n",
      "Epoch: 030, Train Loss: 287.290, Val AUC: 0.500\n",
      "Epoch: 040, Train Loss: 239.014, Val AUC: 0.500\n",
      "Epoch: 050, Train Loss: 197.671, Val AUC: 0.500\n",
      "Epoch: 060, Train Loss: 161.480, Val AUC: 0.500\n",
      "Epoch: 070, Train Loss: 126.565, Val AUC: 0.500\n",
      "Epoch: 080, Train Loss: 92.797, Val AUC: 0.500\n",
      "Epoch: 090, Train Loss: 68.733, Val AUC: 0.500\n",
      "Epoch: 100, Train Loss: 53.414, Val AUC: 0.500\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 1\tTrain Loss: tensor(473.4844, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.5\n",
      "time taken for training :  190.5309293270111\n",
      "time taken for per epoch :  1.905309293270111\n",
      "Test no  6  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  6\n",
      "Epoch: 010, Train Loss: 3.646, Val AUC: 0.583\n",
      "Epoch: 020, Train Loss: 2.854, Val AUC: 0.608\n",
      "Epoch: 030, Train Loss: 2.276, Val AUC: 0.568\n",
      "Epoch: 040, Train Loss: 1.636, Val AUC: 0.558\n",
      "Epoch: 050, Train Loss: 1.088, Val AUC: 0.557\n",
      "Epoch: 060, Train Loss: 0.719, Val AUC: 0.670\n",
      "Epoch: 070, Train Loss: 0.705, Val AUC: 0.734\n",
      "Epoch: 080, Train Loss: 0.702, Val AUC: 0.753\n",
      "Epoch: 090, Train Loss: 0.697, Val AUC: 0.778\n",
      "Epoch: 100, Train Loss: 0.692, Val AUC: 0.804\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 100\tTrain Loss: tensor(0.6920, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8042188897525249\n",
      "time taken for training :  190.14066076278687\n",
      "time taken for per epoch :  1.9014066076278686\n",
      "Test no  7  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  7\n",
      "Epoch: 010, Train Loss: 524.783, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 346.832, Val AUC: 0.500\n",
      "Epoch: 030, Train Loss: 290.717, Val AUC: 0.500\n",
      "Epoch: 040, Train Loss: 254.378, Val AUC: 0.500\n",
      "Epoch: 050, Train Loss: 225.009, Val AUC: 0.500\n",
      "Epoch: 060, Train Loss: 198.933, Val AUC: 0.500\n",
      "Epoch: 070, Train Loss: 175.115, Val AUC: 0.500\n",
      "Epoch: 080, Train Loss: 152.947, Val AUC: 0.500\n",
      "Epoch: 090, Train Loss: 132.078, Val AUC: 0.500\n",
      "Epoch: 100, Train Loss: 112.304, Val AUC: 0.500\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 1\tTrain Loss: tensor(741.5567, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.5\n",
      "time taken for training :  189.00958895683289\n",
      "time taken for per epoch :  1.8900958895683289\n",
      "Test no  8  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  8\n",
      "Epoch: 010, Train Loss: 282.272, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 207.244, Val AUC: 0.500\n",
      "Epoch: 030, Train Loss: 153.261, Val AUC: 0.500\n",
      "Epoch: 040, Train Loss: 110.380, Val AUC: 0.500\n",
      "Epoch: 050, Train Loss: 76.980, Val AUC: 0.500\n",
      "Epoch: 060, Train Loss: 51.619, Val AUC: 0.501\n",
      "Epoch: 070, Train Loss: 33.054, Val AUC: 0.501\n",
      "Epoch: 080, Train Loss: 20.293, Val AUC: 0.504\n",
      "Epoch: 090, Train Loss: 11.987, Val AUC: 0.515\n",
      "Epoch: 100, Train Loss: 6.806, Val AUC: 0.421\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 93\tTrain Loss: tensor(10.1570, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.5254446955859272\n",
      "time taken for training :  188.72301816940308\n",
      "time taken for per epoch :  1.8872301816940307\n",
      "Test no  9  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  9\n",
      "Epoch: 010, Train Loss: 278.973, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 29.061, Val AUC: 0.393\n",
      "Epoch: 030, Train Loss: 8.061, Val AUC: 0.543\n",
      "Epoch: 040, Train Loss: 5.725, Val AUC: 0.540\n",
      "Epoch: 050, Train Loss: 5.071, Val AUC: 0.527\n",
      "Epoch: 060, Train Loss: 4.647, Val AUC: 0.526\n",
      "Epoch: 070, Train Loss: 4.179, Val AUC: 0.536\n",
      "Epoch: 080, Train Loss: 3.804, Val AUC: 0.548\n",
      "Epoch: 090, Train Loss: 3.462, Val AUC: 0.558\n",
      "Epoch: 100, Train Loss: 3.185, Val AUC: 0.567\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 100\tTrain Loss: tensor(3.1845, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.5665293729082519\n",
      "time taken for training :  183.28218007087708\n",
      "time taken for per epoch :  1.8328218007087707\n",
      "Test no  10  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  10\n",
      "Epoch: 010, Train Loss: 575.980, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 460.184, Val AUC: 0.500\n",
      "Epoch: 030, Train Loss: 365.225, Val AUC: 0.500\n",
      "Epoch: 040, Train Loss: 291.838, Val AUC: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_780058/3482021667.py\", line 47, in <module>\n",
      "    best_model,_,_ = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=epochs)\n",
      "  File \"/tmp/ipykernel_780058/892779624.py\", line 40, in train_link_predictor\n",
      "    neg_edge_index = negative_sampling(\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/torch_geometric/utils/negative_sampling.py\", line 107, in negative_sampling\n",
      "    return vector_to_edge_index(neg_idx, size, bipartite, force_undirected)\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/torch_geometric/utils/negative_sampling.py\", line 371, in vector_to_edge_index\n",
      "    col[row <= col] += 1\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1477, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/shellygupta/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 182, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/usr/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/usr/lib/python3.8/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/usr/lib/python3.8/tokenize.py\", line 394, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/usr/lib/python3.8/tokenize.py\", line 363, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/usr/lib/python3.8/tokenize.py\", line 321, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_780058/3482021667.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_link_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mtimetaken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_780058/892779624.py\u001b[0m in \u001b[0;36mtrain_link_predictor\u001b[0;34m(model, train_data, val_data, optimizer, criterion, n_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# sampling training negatives for every training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         neg_edge_index = negative_sampling(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/utils/negative_sampling.py\u001b[0m in \u001b[0;36mnegative_sampling\u001b[0;34m(edge_index, num_nodes, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvector_to_edge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbipartite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_undirected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/utils/negative_sampling.py\u001b[0m in \u001b[0;36mvector_to_edge_index\u001b[0;34m(idx, size, bipartite, force_undirected)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for split_no in range(7,11,1):\n",
    "    test_file_no = split_no\n",
    "    print(\"Test no \", test_file_no, \" going on\")\n",
    "    print(\"\\n\\n\\n Test file no : \",test_file_no)\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in range(1,11,1):\n",
    "        if i != test_file_no:\n",
    "            df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\") \n",
    "            train_df = train_df.append(df, ignore_index=True)\n",
    "            del df\n",
    "        \n",
    "            \n",
    "    train_df = train_df.drop(['layer', 'weight',\"sign\"], axis=1)\n",
    "    \n",
    "    #making node_id variables continuous\n",
    "    temp_node_no = pickle.load(open(\"node_id_into_continuous_node_ids.p\",\"rb\"))\n",
    "    \n",
    "    # collecting all positive edges\n",
    "    train_df['src'] = [temp_node_no[i] for i in train_df['src']]\n",
    "    train_df['dest'] = [temp_node_no[i] for i in train_df['dest']]\n",
    "    final_edge_list = [torch.Tensor([train_df['src']]),torch.Tensor([train_df['dest']])]\n",
    "    final_edge_list = torch.cat(tuple(final_edge_list),dim=0)  \n",
    "    \n",
    "    # downloading  node_embeddings \n",
    "    node_embeddings = pickle.load(open(node_feature_dir_path + \"/node_embeddings_for_pytorch_models_excluding_split_\"+str(test_file_no)+\".p\",\"rb\"))\n",
    "    \n",
    "    #making training and val graphs\n",
    "    data = Data()\n",
    "    data.x = node_embeddings\n",
    "    data.edge_index = final_edge_list.type(torch.int64)\n",
    "\n",
    "    split = T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.0,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=False,\n",
    "        neg_sampling_ratio=1.0)\n",
    "    train_data, val_data, test_data = split(data)\n",
    "    \n",
    "    del train_df, node_embeddings, final_edge_list, data\n",
    "    \n",
    "    model = Net(8, 3, 2) #.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    curr = time.time()\n",
    "    best_model,_,_ = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=epochs)\n",
    "    timetaken = time.time()-curr\n",
    "    print(\"time taken for training : \",timetaken)\n",
    "    print(\"time taken for per epoch : \",timetaken/epochs)\n",
    "    \n",
    "    torch.save(best_model.state_dict(), \"my_\"+str(dbname)+\"_WSJ_GATv2_model_state_dict_\"+str(test_file_no))\n",
    "    torch.save(best_model, \"my_\"+str(dbname)+\"_WSJ_GATv2_whole_model_\"+str(test_file_no))\n",
    "    \n",
    "    del model, train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1201fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no  7  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  7\n",
      "Epoch: 010, Train Loss: 190.579, Val AUC: 0.504\n",
      "Epoch: 020, Train Loss: 140.922, Val AUC: 0.507\n",
      "Epoch: 030, Train Loss: 85.733, Val AUC: 0.513\n",
      "Epoch: 040, Train Loss: 53.159, Val AUC: 0.557\n",
      "Epoch: 050, Train Loss: 22.183, Val AUC: 0.613\n",
      "Epoch: 060, Train Loss: 14.089, Val AUC: 0.631\n",
      "Epoch: 070, Train Loss: 9.815, Val AUC: 0.632\n",
      "Epoch: 080, Train Loss: 7.869, Val AUC: 0.640\n",
      "Epoch: 090, Train Loss: 6.624, Val AUC: 0.651\n",
      "Epoch: 100, Train Loss: 5.647, Val AUC: 0.661\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 100\tTrain Loss: tensor(5.6474, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.6608748369205648\n",
      "time taken for training :  183.69712805747986\n",
      "time taken for per epoch :  1.8369712805747986\n",
      "Test no  8  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  8\n",
      "Epoch: 010, Train Loss: 0.855, Val AUC: 0.559\n",
      "Epoch: 020, Train Loss: 0.721, Val AUC: 0.723\n",
      "Epoch: 030, Train Loss: 0.667, Val AUC: 0.789\n",
      "Epoch: 040, Train Loss: 0.651, Val AUC: 0.772\n",
      "Epoch: 050, Train Loss: 0.638, Val AUC: 0.785\n",
      "Epoch: 060, Train Loss: 0.629, Val AUC: 0.795\n",
      "Epoch: 070, Train Loss: 0.621, Val AUC: 0.794\n",
      "Epoch: 080, Train Loss: 0.615, Val AUC: 0.794\n",
      "Epoch: 090, Train Loss: 0.608, Val AUC: 0.798\n",
      "Epoch: 100, Train Loss: 0.602, Val AUC: 0.800\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 100\tTrain Loss: tensor(0.6020, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.7999470344430484\n",
      "time taken for training :  185.6113350391388\n",
      "time taken for per epoch :  1.856113350391388\n",
      "Test no  9  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  9\n",
      "Epoch: 010, Train Loss: 93.818, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 60.154, Val AUC: 0.500\n",
      "Epoch: 030, Train Loss: 40.654, Val AUC: 0.500\n",
      "Epoch: 040, Train Loss: 28.026, Val AUC: 0.485\n",
      "Epoch: 050, Train Loss: 18.046, Val AUC: 0.463\n",
      "Epoch: 060, Train Loss: 9.324, Val AUC: 0.464\n",
      "Epoch: 070, Train Loss: 4.222, Val AUC: 0.613\n",
      "Epoch: 080, Train Loss: 3.257, Val AUC: 0.644\n",
      "Epoch: 090, Train Loss: 2.630, Val AUC: 0.661\n",
      "Epoch: 100, Train Loss: 1.999, Val AUC: 0.690\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 100\tTrain Loss: tensor(1.9987, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.6901498878059087\n",
      "time taken for training :  185.11193466186523\n",
      "time taken for per epoch :  1.8511193466186524\n",
      "Test no  10  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  10\n",
      "Epoch: 010, Train Loss: 10.598, Val AUC: 0.325\n",
      "Epoch: 020, Train Loss: 3.331, Val AUC: 0.489\n",
      "Epoch: 030, Train Loss: 2.516, Val AUC: 0.394\n",
      "Epoch: 040, Train Loss: 1.978, Val AUC: 0.408\n",
      "Epoch: 050, Train Loss: 1.438, Val AUC: 0.508\n",
      "Epoch: 060, Train Loss: 1.412, Val AUC: 0.608\n",
      "Epoch: 070, Train Loss: 1.370, Val AUC: 0.461\n",
      "Epoch: 080, Train Loss: 1.336, Val AUC: 0.497\n",
      "Epoch: 090, Train Loss: 1.304, Val AUC: 0.516\n",
      "Epoch: 100, Train Loss: 1.294, Val AUC: 0.501\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 60\tTrain Loss: tensor(1.4122, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.6076119261115939\n",
      "time taken for training :  181.8664743900299\n",
      "time taken for per epoch :  1.818664743900299\n"
     ]
    }
   ],
   "source": [
    "for split_no in range(7,11,1):\n",
    "    test_file_no = split_no\n",
    "    print(\"Test no \", test_file_no, \" going on\")\n",
    "    print(\"\\n\\n\\n Test file no : \",test_file_no)\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in range(1,11,1):\n",
    "        if i != test_file_no:\n",
    "            df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\") \n",
    "            train_df = train_df.append(df, ignore_index=True)\n",
    "            del df\n",
    "        \n",
    "            \n",
    "    train_df = train_df.drop(['layer', 'weight',\"sign\"], axis=1)\n",
    "    \n",
    "    #making node_id variables continuous\n",
    "    temp_node_no = pickle.load(open(\"node_id_into_continuous_node_ids.p\",\"rb\"))\n",
    "    \n",
    "    # collecting all positive edges\n",
    "    train_df['src'] = [temp_node_no[i] for i in train_df['src']]\n",
    "    train_df['dest'] = [temp_node_no[i] for i in train_df['dest']]\n",
    "    final_edge_list = [torch.Tensor([train_df['src']]),torch.Tensor([train_df['dest']])]\n",
    "    final_edge_list = torch.cat(tuple(final_edge_list),dim=0)  \n",
    "    \n",
    "    # downloading  node_embeddings \n",
    "    node_embeddings = pickle.load(open(node_feature_dir_path + \"/node_embeddings_for_pytorch_models_excluding_split_\"+str(test_file_no)+\".p\",\"rb\"))\n",
    "    \n",
    "    #making training and val graphs\n",
    "    data = Data()\n",
    "    data.x = node_embeddings\n",
    "    data.edge_index = final_edge_list.type(torch.int64)\n",
    "\n",
    "    split = T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.0,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=False,\n",
    "        neg_sampling_ratio=1.0)\n",
    "    train_data, val_data, test_data = split(data)\n",
    "    \n",
    "    del train_df, node_embeddings, final_edge_list, data\n",
    "    \n",
    "    model = Net(8, 3, 2) #.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    curr = time.time()\n",
    "    best_model,_,_ = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=epochs)\n",
    "    timetaken = time.time()-curr\n",
    "    print(\"time taken for training : \",timetaken)\n",
    "    print(\"time taken for per epoch : \",timetaken/epochs)\n",
    "    \n",
    "    torch.save(best_model.state_dict(), \"my_\"+str(dbname)+\"_WSJ_GATv2_model_state_dict_\"+str(test_file_no))\n",
    "    torch.save(best_model, \"my_\"+str(dbname)+\"_WSJ_GATv2_whole_model_\"+str(test_file_no))\n",
    "    \n",
    "    del model, train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f17f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no  7  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  7\n",
      "Epoch: 010, Train Loss: 0.720, Val AUC: 0.623\n",
      "Epoch: 020, Train Loss: 0.687, Val AUC: 0.666\n",
      "Epoch: 030, Train Loss: 0.677, Val AUC: 0.739\n",
      "Epoch: 040, Train Loss: 0.672, Val AUC: 0.820\n",
      "Epoch: 050, Train Loss: 0.668, Val AUC: 0.844\n",
      "Epoch: 060, Train Loss: 0.662, Val AUC: 0.850\n",
      "Epoch: 070, Train Loss: 0.654, Val AUC: 0.853\n",
      "Epoch: 080, Train Loss: 0.632, Val AUC: 0.862\n",
      "Epoch: 090, Train Loss: 0.607, Val AUC: 0.866\n",
      "Epoch: 100, Train Loss: 0.591, Val AUC: 0.867\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 86\tTrain Loss: tensor(0.6139, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8678133643194659\n",
      "time taken for training :  184.82191967964172\n",
      "time taken for per epoch :  1.8482191967964172\n",
      "Test no  9  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  9\n",
      "Epoch: 010, Train Loss: 12.625, Val AUC: 0.491\n",
      "Epoch: 020, Train Loss: 9.137, Val AUC: 0.469\n",
      "Epoch: 030, Train Loss: 6.394, Val AUC: 0.520\n",
      "Epoch: 040, Train Loss: 4.275, Val AUC: 0.535\n",
      "Epoch: 050, Train Loss: 2.816, Val AUC: 0.514\n",
      "Epoch: 060, Train Loss: 1.893, Val AUC: 0.498\n",
      "Epoch: 070, Train Loss: 1.373, Val AUC: 0.550\n",
      "Epoch: 080, Train Loss: 1.051, Val AUC: 0.628\n",
      "Epoch: 090, Train Loss: 0.843, Val AUC: 0.669\n",
      "Epoch: 100, Train Loss: 0.736, Val AUC: 0.725\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 100\tTrain Loss: tensor(0.7355, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.724668173269075\n",
      "time taken for training :  188.94207859039307\n",
      "time taken for per epoch :  1.8894207859039307\n",
      "Test no  10  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  10\n",
      "Epoch: 010, Train Loss: 3.416, Val AUC: 0.371\n",
      "Epoch: 020, Train Loss: 2.424, Val AUC: 0.374\n",
      "Epoch: 030, Train Loss: 1.681, Val AUC: 0.376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_780058/3745550614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_link_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mtimetaken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time taken for training : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimetaken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_780058/892779624.py\u001b[0m in \u001b[0;36mtrain_link_predictor\u001b[0;34m(model, train_data, val_data, optimizer, criterion, n_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# sampling training negatives for every training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         neg_edge_index = negative_sampling(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/utils/negative_sampling.py\u001b[0m in \u001b[0;36mnegative_sampling\u001b[0;34m(edge_index, num_nodes, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Number of tries to sample negative indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mneg_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \"\"\"\n\u001b[1;32m    889\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m     return in1d(element, test_elements, assume_unique=assume_unique,\n\u001b[0m\u001b[1;32m    891\u001b[0m                 invert=invert, kind=kind).reshape(element.shape)\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0mar2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         ret = _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0m\u001b[1;32m    275\u001b[0m                         equal_nan=equal_nan)\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for split_no in [7,9,10]:\n",
    "    test_file_no = split_no\n",
    "    print(\"Test no \", test_file_no, \" going on\")\n",
    "    print(\"\\n\\n\\n Test file no : \",test_file_no)\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in range(1,11,1):\n",
    "        if i != test_file_no:\n",
    "            df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\") \n",
    "            train_df = train_df.append(df, ignore_index=True)\n",
    "            del df\n",
    "        \n",
    "            \n",
    "    train_df = train_df.drop(['layer', 'weight',\"sign\"], axis=1)\n",
    "    \n",
    "    #making node_id variables continuous\n",
    "    temp_node_no = pickle.load(open(\"node_id_into_continuous_node_ids.p\",\"rb\"))\n",
    "    \n",
    "    # collecting all positive edges\n",
    "    train_df['src'] = [temp_node_no[i] for i in train_df['src']]\n",
    "    train_df['dest'] = [temp_node_no[i] for i in train_df['dest']]\n",
    "    final_edge_list = [torch.Tensor([train_df['src']]),torch.Tensor([train_df['dest']])]\n",
    "    final_edge_list = torch.cat(tuple(final_edge_list),dim=0)  \n",
    "    \n",
    "    # downloading  node_embeddings \n",
    "    node_embeddings = pickle.load(open(node_feature_dir_path + \"/node_embeddings_for_pytorch_models_excluding_split_\"+str(test_file_no)+\".p\",\"rb\"))\n",
    "    \n",
    "    #making training and val graphs\n",
    "    data = Data()\n",
    "    data.x = node_embeddings\n",
    "    data.edge_index = final_edge_list.type(torch.int64)\n",
    "\n",
    "    split = T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.0,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=False,\n",
    "        neg_sampling_ratio=1.0)\n",
    "    train_data, val_data, test_data = split(data)\n",
    "    \n",
    "    del train_df, node_embeddings, final_edge_list, data\n",
    "    \n",
    "    model = Net(8, 3, 2) #.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    curr = time.time()\n",
    "    best_model,_,_ = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=epochs)\n",
    "    timetaken = time.time()-curr\n",
    "    print(\"time taken for training : \",timetaken)\n",
    "    print(\"time taken for per epoch : \",timetaken/epochs)\n",
    "    \n",
    "    torch.save(best_model.state_dict(), \"my_\"+str(dbname)+\"_last_WSJ_GATv2_model_state_dict_\"+str(test_file_no))\n",
    "    torch.save(best_model, \"my_\"+str(dbname)+\"_last_WSJ_GATv2_whole_model_\"+str(test_file_no))\n",
    "    \n",
    "    del model, train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6388eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no  9  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  9\n",
      "Epoch: 010, Train Loss: 0.681, Val AUC: 0.745\n",
      "Epoch: 020, Train Loss: 0.607, Val AUC: 0.870\n",
      "Epoch: 030, Train Loss: 0.576, Val AUC: 0.866\n",
      "Epoch: 040, Train Loss: 0.559, Val AUC: 0.861\n",
      "Epoch: 050, Train Loss: 0.548, Val AUC: 0.861\n",
      "Epoch: 060, Train Loss: 0.541, Val AUC: 0.863\n",
      "Epoch: 070, Train Loss: 0.537, Val AUC: 0.866\n",
      "Epoch: 080, Train Loss: 0.536, Val AUC: 0.867\n",
      "Epoch: 090, Train Loss: 0.534, Val AUC: 0.869\n",
      "Epoch: 100, Train Loss: 0.533, Val AUC: 0.870\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 21\tTrain Loss: tensor(0.6026, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8698653630849256\n",
      "time taken for training :  188.95362830162048\n",
      "time taken for per epoch :  1.8895362830162048\n",
      "Test no  10  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  10\n",
      "Epoch: 010, Train Loss: 663.670, Val AUC: 0.500\n",
      "Epoch: 020, Train Loss: 537.002, Val AUC: 0.500\n",
      "Epoch: 030, Train Loss: 431.600, Val AUC: 0.500\n",
      "Epoch: 040, Train Loss: 345.106, Val AUC: 0.500\n",
      "Epoch: 050, Train Loss: 274.962, Val AUC: 0.500\n",
      "Epoch: 060, Train Loss: 218.097, Val AUC: 0.500\n",
      "Epoch: 070, Train Loss: 172.164, Val AUC: 0.500\n",
      "Epoch: 080, Train Loss: 134.147, Val AUC: 0.501\n",
      "Epoch: 090, Train Loss: 97.113, Val AUC: 0.502\n",
      "Epoch: 100, Train Loss: 42.446, Val AUC: 0.524\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 100\tTrain Loss: tensor(42.4463, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.5235955776052293\n",
      "time taken for training :  186.54834699630737\n",
      "time taken for per epoch :  1.8654834699630738\n"
     ]
    }
   ],
   "source": [
    "for split_no in [9,10]:\n",
    "    test_file_no = split_no\n",
    "    print(\"Test no \", test_file_no, \" going on\")\n",
    "    print(\"\\n\\n\\n Test file no : \",test_file_no)\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in range(1,11,1):\n",
    "        if i != test_file_no:\n",
    "            df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\") \n",
    "            train_df = train_df.append(df, ignore_index=True)\n",
    "            del df\n",
    "        \n",
    "            \n",
    "    train_df = train_df.drop(['layer', 'weight',\"sign\"], axis=1)\n",
    "    \n",
    "    #making node_id variables continuous\n",
    "    temp_node_no = pickle.load(open(\"node_id_into_continuous_node_ids.p\",\"rb\"))\n",
    "    \n",
    "    # collecting all positive edges\n",
    "    train_df['src'] = [temp_node_no[i] for i in train_df['src']]\n",
    "    train_df['dest'] = [temp_node_no[i] for i in train_df['dest']]\n",
    "    final_edge_list = [torch.Tensor([train_df['src']]),torch.Tensor([train_df['dest']])]\n",
    "    final_edge_list = torch.cat(tuple(final_edge_list),dim=0)  \n",
    "    \n",
    "    # downloading  node_embeddings \n",
    "    node_embeddings = pickle.load(open(node_feature_dir_path + \"/node_embeddings_for_pytorch_models_excluding_split_\"+str(test_file_no)+\".p\",\"rb\"))\n",
    "    \n",
    "    #making training and val graphs\n",
    "    data = Data()\n",
    "    data.x = node_embeddings\n",
    "    data.edge_index = final_edge_list.type(torch.int64)\n",
    "\n",
    "    split = T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.0,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=False,\n",
    "        neg_sampling_ratio=1.0)\n",
    "    train_data, val_data, test_data = split(data)\n",
    "    \n",
    "    del train_df, node_embeddings, final_edge_list, data\n",
    "    \n",
    "    model = Net(8, 3, 2) #.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    curr = time.time()\n",
    "    best_model,_,_ = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=epochs)\n",
    "    timetaken = time.time()-curr\n",
    "    print(\"time taken for training : \",timetaken)\n",
    "    print(\"time taken for per epoch : \",timetaken/epochs)\n",
    "    \n",
    "    torch.save(best_model.state_dict(), \"my_\"+str(dbname)+\"_last_WSJ_GATv2_model_state_dict_\"+str(test_file_no))\n",
    "    torch.save(best_model, \"my_\"+str(dbname)+\"_last_WSJ_GATv2_whole_model_\"+str(test_file_no))\n",
    "    \n",
    "    del model, train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8eb310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no  10  going on\n",
      "\n",
      "\n",
      "\n",
      " Test file no :  10\n",
      "Epoch: 010, Train Loss: 0.623, Val AUC: 0.859\n",
      "Epoch: 020, Train Loss: 0.600, Val AUC: 0.864\n",
      "Epoch: 030, Train Loss: 0.575, Val AUC: 0.866\n",
      "Epoch: 040, Train Loss: 0.555, Val AUC: 0.867\n",
      "Epoch: 050, Train Loss: 0.541, Val AUC: 0.869\n",
      "Epoch: 060, Train Loss: 0.534, Val AUC: 0.869\n",
      "Epoch: 070, Train Loss: 0.532, Val AUC: 0.869\n",
      "Epoch: 080, Train Loss: 0.531, Val AUC: 0.870\n",
      "Epoch: 090, Train Loss: 0.532, Val AUC: 0.870\n",
      "Epoch: 100, Train Loss: 0.530, Val AUC: 0.871\n",
      "Epoch: 110, Train Loss: 0.530, Val AUC: 0.871\n",
      "Epoch: 120, Train Loss: 0.530, Val AUC: 0.872\n",
      "Epoch: 130, Train Loss: 0.530, Val AUC: 0.872\n",
      "Epoch: 140, Train Loss: 0.530, Val AUC: 0.872\n",
      "Epoch: 150, Train Loss: 0.530, Val AUC: 0.872\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 150\tTrain Loss: tensor(0.5299, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8722163302107182\n",
      "time taken for training :  293.67836475372314\n",
      "time taken for per epoch :  1.957855765024821\n"
     ]
    }
   ],
   "source": [
    "for split_no in [10]:\n",
    "    test_file_no = split_no\n",
    "    print(\"Test no \", test_file_no, \" going on\")\n",
    "    print(\"\\n\\n\\n Test file no : \",test_file_no)\n",
    "    train_df = pd.DataFrame()\n",
    "    for i in range(1,11,1):\n",
    "        if i != test_file_no:\n",
    "            df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\") \n",
    "            train_df = train_df.append(df, ignore_index=True)\n",
    "            del df\n",
    "        \n",
    "            \n",
    "    train_df = train_df.drop(['layer', 'weight',\"sign\"], axis=1)\n",
    "    \n",
    "    #making node_id variables continuous\n",
    "    temp_node_no = pickle.load(open(\"node_id_into_continuous_node_ids.p\",\"rb\"))\n",
    "    \n",
    "    # collecting all positive edges\n",
    "    train_df['src'] = [temp_node_no[i] for i in train_df['src']]\n",
    "    train_df['dest'] = [temp_node_no[i] for i in train_df['dest']]\n",
    "    final_edge_list = [torch.Tensor([train_df['src']]),torch.Tensor([train_df['dest']])]\n",
    "    final_edge_list = torch.cat(tuple(final_edge_list),dim=0)  \n",
    "    \n",
    "    # downloading  node_embeddings \n",
    "    node_embeddings = pickle.load(open(node_feature_dir_path + \"/node_embeddings_for_pytorch_models_excluding_split_\"+str(test_file_no)+\".p\",\"rb\"))\n",
    "    \n",
    "    #making training and val graphs\n",
    "    data = Data()\n",
    "    data.x = node_embeddings\n",
    "    data.edge_index = final_edge_list.type(torch.int64)\n",
    "\n",
    "    split = T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.0,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=False,\n",
    "        neg_sampling_ratio=1.0)\n",
    "    train_data, val_data, test_data = split(data)\n",
    "    \n",
    "    del train_df, node_embeddings, final_edge_list, data\n",
    "    \n",
    "    model = Net(8, 3, 2) #.to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    curr = time.time()\n",
    "    best_model,_,_ = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=150)\n",
    "    timetaken = time.time()-curr\n",
    "    print(\"time taken for training : \",timetaken)\n",
    "    print(\"time taken for per epoch : \",timetaken/150)\n",
    "    \n",
    "    torch.save(best_model.state_dict(), \"my_\"+str(dbname)+\"_last_WSJ_GATv2_model_state_dict_\"+str(test_file_no))\n",
    "    torch.save(best_model, \"my_\"+str(dbname)+\"_last_WSJ_GATv2_whole_model_\"+str(test_file_no))\n",
    "    \n",
    "    del model, train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5356454",
   "metadata": {},
   "source": [
    "# create node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb23eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_node_no = pickle.load(open(\"node_id_into_continuous_node_ids.p\",\"rb\"))\n",
    "for split_no in range(1,11,1):\n",
    "    test_file_no = split_no\n",
    "    node_embeddings = pd.read_csv(node_feature_dir_path + \"/node_features_excluding_split_\"+str(test_file_no)+\".csv\")\n",
    "    node_embeddings[\"node_id\"] = [temp_node_no[i] for i in node_embeddings[\"node_id\"]]\n",
    "    node_embeddings = node_embeddings.sort_values(by = 'node_id')\n",
    "    column = \"1\"\n",
    "    node_embeddings[column] = (node_embeddings[column] - node_embeddings[column].mean()) / node_embeddings[column].std() \n",
    "    column = \"2\"\n",
    "    node_embeddings[column] = (node_embeddings[column] - node_embeddings[column].mean()) / node_embeddings[column].std() \n",
    "    column = \"3\"\n",
    "    node_embeddings[column] = (node_embeddings[column] - node_embeddings[column].mean()) / node_embeddings[column].std() \n",
    "    column = \"12\"\n",
    "    node_embeddings[column] = (node_embeddings[column] - node_embeddings[column].mean()) / node_embeddings[column].std() \n",
    "    column = \"32\"\n",
    "    node_embeddings[column] = (node_embeddings[column] - node_embeddings[column].mean()) / node_embeddings[column].std()\n",
    "    \n",
    "    node_embeddings = node_embeddings.set_index('node_id')\n",
    "    df = node_embeddings.copy()\n",
    "    node_features = []\n",
    "    for index, row in df.iterrows():\n",
    "        node_features.append(torch.Tensor([list(row)]))\n",
    "\n",
    "    node_features = torch.cat(tuple(node_features),dim=0)  \n",
    "    \n",
    "    pickle.dump(node_features,open(node_feature_dir_path + \"/node_embeddings_for_pytorch_models_excluding_split_\"+str(test_file_no)+\".p\",\"wb\"))\n",
    "    del node_embeddings,node_features,df\n",
    "    \n",
    "    #making graph\n",
    "    data = Data()\n",
    "    data.x = node_embeddings\n",
    "    data.edge_index = final_edge_list.type(torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551c605",
   "metadata": {},
   "source": [
    "# rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "import random#user set params\n",
    "dir_path = \"WSJ split data files\"\n",
    "dbname = \"WSJ\"\n",
    "epochs = 100\n",
    "\n",
    "#variables required\n",
    "flatten_file_name = 'WSJ_comm_flatten.csv'\n",
    "dbname = \"WSJ\"\n",
    "\n",
    "#read flatten file\n",
    "df = pd.read_csv(flatten_file_name) \n",
    "df = df.drop(['time', 'article_id',\"comment_id\"], axis=1)\n",
    "\n",
    "# converting nodes to continuous variable\n",
    "temp_node_no = {}\n",
    "id_to_node = {}\n",
    "id = 0\n",
    "for i in np.unique(df[['src', 'dest']].values):\n",
    "    temp_node_no[i] = id\n",
    "    id_to_node[id] = i\n",
    "    id += 1\n",
    "print(\"total unique nodes found : \",id)\n",
    "\n",
    "# collecting all positive edges\n",
    "df['new_src'] = [temp_node_no[i] for i in df['src']]\n",
    "df['new_dest'] = [temp_node_no[i] for i in df['dest']]\n",
    "positive_edges = [torch.Tensor([[df['new_src'][i],df['new_dest'][i]]]) for i in df.index]\n",
    "positive_edges = torch.cat(tuple(positive_edges),dim=0)\n",
    "print(\"len of df : \",len(df))\n",
    "print(\"len of edges before removing duplicates : \",len(positive_edges))\n",
    "\n",
    "#checking for and removing duplicates\n",
    "positive_edges = torch.unique(positive_edges,dim=0)\n",
    "print(\"Total unique edges : \",len(positive_edges))\n",
    "\n",
    "# diving positive into 10 split\n",
    "df = df.drop(['new_src', 'new_dest'], axis=1)                   \n",
    "df[\"label\"] = [1 for i in range(len(df))]\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "positive_split = np.array_split(df, 10)\n",
    "for i in range(len(positive_split)):\n",
    "    file_name = str(dbname)+\"_positive_edges_split_\"+str(i+1)+\".csv\"\n",
    "    positive_split[i].to_csv(file_name, encoding='utf-8', index=False)                   \n",
    "print(\"Positive edges successfully divided into 10 partitions\")\n",
    "del df, positive_split\n",
    "\n",
    "#finally generating negative edges\n",
    "negative_edges = torch_geometric.utils.negative_sampling(edge_index = positive_edges.T)\n",
    "negative_edges = negative_edges.tolist()\n",
    "neg_df = pd.DataFrame(negative_edges, columns = ['src', 'dest'])\n",
    "del negative_edges, positive_edges\n",
    "neg_df[\"label\"] = [-1 for i in range(len(neg_df))]\n",
    "print(\"No of negative edges generated : \",len(neg_df))\n",
    "                   \n",
    "# converting neg_df into predecided node ids\n",
    "new_src= [id_to_node[i] for i in neg_df['src']]\n",
    "new_dest = [id_to_node[i] for i in neg_df['dest']]\n",
    "neg_df['src'] = new_src\n",
    "neg_df['dest'] = new_dest \n",
    "del new_src, new_dest                   \n",
    "neg_df[\"weight\"] = [random.randint(1,301) for counter in range(len(neg_df))]\n",
    "neg_df[\"sign\"] = [random.randint(-2,2) for counter in range(len(neg_df))]\n",
    "neg_df = neg_df.loc[:,['src','dest','weight','sign','label']]  \n",
    "print(\"Negative edges df successfully created\")\n",
    "                   \n",
    "# diving negative edges into 10 splits\n",
    "neg_df = neg_df.sample(frac=1).reset_index(drop=True)\n",
    "negative_split = np.array_split(neg_df, 10)\n",
    "for i in range(len(negative_split)):\n",
    "    file_name = str(dbname)+\"_negative_edges_split_\"+str(i+1)+\".csv\"\n",
    "    negative_split[i].to_csv(file_name, encoding='utf-8', index=False)\n",
    "print(\"Negative edges successfully divided into 10 partitions\")\n",
    "del neg_df, negative_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb41f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import time \n",
    "import torch\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, accuracy_score,f1_score,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b3886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8af58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GAT run this else run next block\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels,heads = heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(\n",
    "            dim=-1\n",
    "        )  # product of a pair of nodes on each edge\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "    \n",
    "\n",
    "def train_link_predictor(\n",
    "    model, train_data, val_data, optimizer, criterion, n_epochs=100\n",
    "):\n",
    "    best_model = model\n",
    "    best_val_auc = 0\n",
    "    val_auc_list = []\n",
    "    train_loss_list = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "        # sampling training negatives for every training epoch\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "            num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        edge_label_index = torch.cat(\n",
    "            [train_data.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        edge_label = torch.cat([\n",
    "            train_data.edge_label,\n",
    "            train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_auc = eval_link_predictor(model, val_data)\n",
    "        val_auc_list.append(val_auc)\n",
    "        if (val_auc > best_val_auc):\n",
    "            print(\"best_model at epoch : \",epoch)\n",
    "            torch.save(model,\"best_link_prediction.pt\")\n",
    "            best_model = model\n",
    "            best_val_auc = val_auc\n",
    "            line = \"Epoch: \"+str(epoch) + \"\\tTrain Loss: \"+str(loss) + \"\\tVal AUC: \"+str(best_val_auc)\n",
    "            \n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val AUC: {val_auc:.3f}\")\n",
    "        #train_loss_list.append({loss:.3f})\n",
    "        train_loss_list.append(loss)\n",
    "    print(\"\\n\\n for best model :\\n\",line)\n",
    "    \n",
    "    return best_model,val_auc_list,train_loss_list #,train_auc_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, data):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55456fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"WSJ split data files\"\n",
    "test_file_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1caedaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ae5d777dec2f>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_df = test_df.append(neg_df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_df = test_df.append(neg_df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dest</th>\n",
       "      <th>layer</th>\n",
       "      <th>weight</th>\n",
       "      <th>sign</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5046</td>\n",
       "      <td>8395</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100036</td>\n",
       "      <td>103501</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100670</td>\n",
       "      <td>102593</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>870</td>\n",
       "      <td>5764</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5036</td>\n",
       "      <td>6048</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569695</th>\n",
       "      <td>100442</td>\n",
       "      <td>100451</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569696</th>\n",
       "      <td>5338</td>\n",
       "      <td>8134</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569697</th>\n",
       "      <td>100075</td>\n",
       "      <td>100086</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569698</th>\n",
       "      <td>5082</td>\n",
       "      <td>5400</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569699</th>\n",
       "      <td>101385</td>\n",
       "      <td>103235</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569700 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           src    dest  layer  weight  sign  label\n",
       "0         5046    8395     12       2  -2.0      1\n",
       "1       100036  103501      3      27  -2.0      1\n",
       "2       100670  102593      3       2  -2.0      1\n",
       "3          870    5764      2      33  -1.0      1\n",
       "4         5036    6048     12       7  -2.0      1\n",
       "...        ...     ...    ...     ...   ...    ...\n",
       "569695  100442  100451      3       4  -2.0      1\n",
       "569696    5338    8134     12       3  -2.0      1\n",
       "569697  100075  100086      3       5  -2.0      1\n",
       "569698    5082    5400     12       2  -2.0      1\n",
       "569699  101385  103235      3       3  -2.0      1\n",
       "\n",
       "[569700 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ae5d777dec2f>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dest</th>\n",
       "      <th>layer</th>\n",
       "      <th>weight</th>\n",
       "      <th>sign</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5046</td>\n",
       "      <td>8395</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100036</td>\n",
       "      <td>103501</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100670</td>\n",
       "      <td>102593</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>870</td>\n",
       "      <td>5764</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5036</td>\n",
       "      <td>6048</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569695</th>\n",
       "      <td>100442</td>\n",
       "      <td>100451</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569696</th>\n",
       "      <td>5338</td>\n",
       "      <td>8134</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569697</th>\n",
       "      <td>100075</td>\n",
       "      <td>100086</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569698</th>\n",
       "      <td>5082</td>\n",
       "      <td>5400</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569699</th>\n",
       "      <td>101385</td>\n",
       "      <td>103235</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569700 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           src    dest  layer  weight  sign  label\n",
       "0         5046    8395     12       2  -2.0      1\n",
       "1       100036  103501      3      27  -2.0      1\n",
       "2       100670  102593      3       2  -2.0      1\n",
       "3          870    5764      2      33  -1.0      1\n",
       "4         5036    6048     12       7  -2.0      1\n",
       "...        ...     ...    ...     ...   ...    ...\n",
       "569695  100442  100451      3       4  -2.0      1\n",
       "569696    5338    8134     12       3  -2.0      1\n",
       "569697  100075  100086      3       5  -2.0      1\n",
       "569698    5082    5400     12       2  -2.0      1\n",
       "569699  101385  103235      3       3  -2.0      1\n",
       "\n",
       "[569700 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "for i in range(1,11,1):\n",
    "    if i != test_file_no:\n",
    "        df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\") \n",
    "        train_df = train_df.append(df, ignore_index=True)\n",
    "        del df\n",
    "    else:\n",
    "        test_df = pd.read_csv(dir_path+\"/WSJ_positive_edges_split_\"+str(i)+\".csv\")\n",
    "        neg_df = pd.read_csv(dir_path+\"/WSJ_negative_edges_split_\"+str(i)+\".csv\")\n",
    "        test_df = test_df.append(neg_df, ignore_index=True)\n",
    "        del neg_df\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a62cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['layer', 'weight',\"sign\"], axis=1)\n",
    "test_df = test_df.drop(['layer', 'weight',\"sign\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4454a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique nodes found till train_df:  8536\n",
      "total unique nodes found till train_df:  8536\n"
     ]
    }
   ],
   "source": [
    "# converting nodes to continuous variable\n",
    "temp_node_no = {}\n",
    "id = 0\n",
    "node_embeddings = []\n",
    "for i in np.unique(train_df[['src', 'dest']].values):\n",
    "    temp_node_no[i] = id\n",
    "    if temp_node_no[i]<5000:\n",
    "        node_embeddings.append(torch.Tensor([[1,0,0]]))\n",
    "    elif temp_node_no[i]<100000:\n",
    "        node_embeddings.append(torch.Tensor([[0,1,0]]))\n",
    "    else:\n",
    "        node_embeddings.append(torch.Tensor([[0,0,1]]))\n",
    "    id += 1\n",
    "    \n",
    "node_embeddings = torch.cat(tuple(node_embeddings),dim=0)  \n",
    "\n",
    "print(\"total unique nodes found till train_df: \",id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e092eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['src'] = [temp_node_no[i] for i in train_df['src']]\n",
    "train_df['dest'] = [temp_node_no[i] for i in train_df['dest']]\n",
    "final_edge_list = [torch.Tensor([train_df['src']]),torch.Tensor([train_df['dest']])]\n",
    "final_edge_list = torch.cat(tuple(final_edge_list),dim=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf279b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  74., 4921., 5555.,  ..., 4960.,  110., 6270.],\n",
       "        [3423., 8386., 7478.,  ..., 4971.,  428., 8120.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  74., 4921., 5555.,  ..., 4960.,  110., 6270.],\n",
       "        [3423., 8386., 7478.,  ..., 4971.,  428., 8120.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a069427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.x = node_embeddings\n",
    "data.edge_index = final_edge_list.type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12fa0ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87d9f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[8536, 3], edge_index=[2, 569700])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[8536, 3], edge_index=[2, 569700])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63958e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = T.RandomLinkSplit(\n",
    "    num_val=0.05,\n",
    "    num_test=0.0,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=False,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "train_data, val_data, test_data = split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db35f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[8536, 3], edge_index=[2, 1082430], edge_label=[541215], edge_label_index=[2, 541215])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[8536, 3], edge_index=[2, 1082430], edge_label=[541215], edge_label_index=[2, 541215])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b21bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df,node_embeddings,final_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e5a308",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dc2b4a6c2d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'node_embeddings' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'node_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dc2b4a6c2d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'node_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "del node_embeddings, train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fa8f11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model at epoch :  1\n",
      "best_model at epoch :  1\n",
      "best_model at epoch :  2\n",
      "best_model at epoch :  2\n",
      "best_model at epoch :  3\n",
      "best_model at epoch :  3\n",
      "best_model at epoch :  4\n",
      "best_model at epoch :  4\n",
      "best_model at epoch :  5\n",
      "best_model at epoch :  5\n",
      "best_model at epoch :  6\n",
      "best_model at epoch :  6\n",
      "best_model at epoch :  7\n",
      "best_model at epoch :  7\n",
      "best_model at epoch :  8\n",
      "best_model at epoch :  8\n",
      "best_model at epoch :  9\n",
      "best_model at epoch :  9\n",
      "best_model at epoch :  10\n",
      "Epoch: 010, Train Loss: 0.702, Val AUC: 0.289\n",
      "best_model at epoch :  10\n",
      "Epoch: 010, Train Loss: 0.702, Val AUC: 0.289\n",
      "best_model at epoch :  11\n",
      "best_model at epoch :  11\n",
      "best_model at epoch :  12\n",
      "best_model at epoch :  12\n",
      "best_model at epoch :  13\n",
      "best_model at epoch :  13\n",
      "best_model at epoch :  14\n",
      "best_model at epoch :  14\n",
      "best_model at epoch :  15\n",
      "best_model at epoch :  15\n",
      "best_model at epoch :  16\n",
      "best_model at epoch :  16\n",
      "best_model at epoch :  17\n",
      "best_model at epoch :  17\n",
      "best_model at epoch :  18\n",
      "best_model at epoch :  18\n",
      "best_model at epoch :  19\n",
      "best_model at epoch :  19\n",
      "best_model at epoch :  20\n",
      "Epoch: 020, Train Loss: 0.696, Val AUC: 0.632\n",
      "best_model at epoch :  20\n",
      "Epoch: 020, Train Loss: 0.696, Val AUC: 0.632\n",
      "best_model at epoch :  21\n",
      "best_model at epoch :  21\n",
      "best_model at epoch :  22\n",
      "best_model at epoch :  22\n",
      "best_model at epoch :  23\n",
      "best_model at epoch :  23\n",
      "best_model at epoch :  24\n",
      "best_model at epoch :  24\n",
      "best_model at epoch :  25\n",
      "best_model at epoch :  25\n",
      "best_model at epoch :  26\n",
      "best_model at epoch :  26\n",
      "best_model at epoch :  27\n",
      "best_model at epoch :  27\n",
      "best_model at epoch :  28\n",
      "best_model at epoch :  28\n",
      "best_model at epoch :  29\n",
      "best_model at epoch :  29\n",
      "best_model at epoch :  30\n",
      "Epoch: 030, Train Loss: 0.691, Val AUC: 0.643\n",
      "best_model at epoch :  30\n",
      "Epoch: 030, Train Loss: 0.691, Val AUC: 0.643\n",
      "best_model at epoch :  31\n",
      "best_model at epoch :  31\n",
      "best_model at epoch :  32\n",
      "best_model at epoch :  32\n",
      "best_model at epoch :  33\n",
      "best_model at epoch :  33\n",
      "best_model at epoch :  34\n",
      "best_model at epoch :  34\n",
      "best_model at epoch :  35\n",
      "best_model at epoch :  35\n",
      "best_model at epoch :  36\n",
      "best_model at epoch :  36\n",
      "best_model at epoch :  37\n",
      "best_model at epoch :  37\n",
      "best_model at epoch :  38\n",
      "best_model at epoch :  38\n",
      "best_model at epoch :  39\n",
      "best_model at epoch :  39\n",
      "best_model at epoch :  40\n",
      "Epoch: 040, Train Loss: 0.685, Val AUC: 0.695\n",
      "best_model at epoch :  40\n",
      "Epoch: 040, Train Loss: 0.685, Val AUC: 0.695\n",
      "best_model at epoch :  41\n",
      "best_model at epoch :  41\n",
      "best_model at epoch :  42\n",
      "best_model at epoch :  42\n",
      "best_model at epoch :  43\n",
      "best_model at epoch :  43\n",
      "best_model at epoch :  44\n",
      "best_model at epoch :  44\n",
      "best_model at epoch :  45\n",
      "best_model at epoch :  45\n",
      "best_model at epoch :  46\n",
      "best_model at epoch :  46\n",
      "best_model at epoch :  47\n",
      "best_model at epoch :  47\n",
      "Epoch: 050, Train Loss: 0.677, Val AUC: 0.838\n",
      "Epoch: 050, Train Loss: 0.677, Val AUC: 0.838\n",
      "best_model at epoch :  52\n",
      "best_model at epoch :  52\n",
      "best_model at epoch :  53\n",
      "best_model at epoch :  53\n",
      "best_model at epoch :  54\n",
      "best_model at epoch :  54\n",
      "best_model at epoch :  55\n",
      "best_model at epoch :  55\n",
      "best_model at epoch :  56\n",
      "best_model at epoch :  56\n",
      "best_model at epoch :  57\n",
      "best_model at epoch :  57\n",
      "best_model at epoch :  58\n",
      "best_model at epoch :  58\n",
      "best_model at epoch :  59\n",
      "best_model at epoch :  59\n",
      "Epoch: 060, Train Loss: 0.669, Val AUC: 0.840\n",
      "Epoch: 060, Train Loss: 0.669, Val AUC: 0.840\n",
      "Epoch: 070, Train Loss: 0.659, Val AUC: 0.837\n",
      "Epoch: 070, Train Loss: 0.659, Val AUC: 0.837\n",
      "Epoch: 080, Train Loss: 0.649, Val AUC: 0.832\n",
      "Epoch: 080, Train Loss: 0.649, Val AUC: 0.832\n",
      "Epoch: 090, Train Loss: 0.638, Val AUC: 0.824\n",
      "Epoch: 090, Train Loss: 0.638, Val AUC: 0.824\n",
      "Epoch: 100, Train Loss: 0.627, Val AUC: 0.816\n",
      "Epoch: 100, Train Loss: 0.627, Val AUC: 0.816\n",
      "Epoch: 110, Train Loss: 0.617, Val AUC: 0.815\n",
      "Epoch: 110, Train Loss: 0.617, Val AUC: 0.815\n",
      "Epoch: 120, Train Loss: 0.608, Val AUC: 0.819\n",
      "Epoch: 120, Train Loss: 0.608, Val AUC: 0.819\n",
      "Epoch: 130, Train Loss: 0.599, Val AUC: 0.826\n",
      "Epoch: 130, Train Loss: 0.599, Val AUC: 0.826\n",
      "Epoch: 140, Train Loss: 0.591, Val AUC: 0.831\n",
      "Epoch: 140, Train Loss: 0.591, Val AUC: 0.831\n",
      "Epoch: 150, Train Loss: 0.582, Val AUC: 0.834\n",
      "Epoch: 150, Train Loss: 0.582, Val AUC: 0.834\n",
      "Epoch: 160, Train Loss: 0.574, Val AUC: 0.836\n",
      "Epoch: 160, Train Loss: 0.574, Val AUC: 0.836\n",
      "Epoch: 170, Train Loss: 0.567, Val AUC: 0.837\n",
      "Epoch: 170, Train Loss: 0.567, Val AUC: 0.837\n",
      "Epoch: 180, Train Loss: 0.561, Val AUC: 0.838\n",
      "Epoch: 180, Train Loss: 0.561, Val AUC: 0.838\n",
      "Epoch: 190, Train Loss: 0.555, Val AUC: 0.838\n",
      "Epoch: 190, Train Loss: 0.555, Val AUC: 0.838\n",
      "Epoch: 200, Train Loss: 0.551, Val AUC: 0.838\n",
      "Epoch: 200, Train Loss: 0.551, Val AUC: 0.838\n",
      "Epoch: 210, Train Loss: 0.547, Val AUC: 0.839\n",
      "Epoch: 210, Train Loss: 0.547, Val AUC: 0.839\n",
      "Epoch: 220, Train Loss: 0.545, Val AUC: 0.838\n",
      "Epoch: 220, Train Loss: 0.545, Val AUC: 0.838\n",
      "Epoch: 230, Train Loss: 0.543, Val AUC: 0.838\n",
      "Epoch: 230, Train Loss: 0.543, Val AUC: 0.838\n",
      "Epoch: 240, Train Loss: 0.542, Val AUC: 0.838\n",
      "Epoch: 240, Train Loss: 0.542, Val AUC: 0.838\n",
      "Epoch: 250, Train Loss: 0.540, Val AUC: 0.838\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 59\tTrain Loss: tensor(0.6699, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8400272764730653\n",
      "time taken for training :  568.3860099315643\n",
      "Epoch: 250, Train Loss: 0.540, Val AUC: 0.838\n",
      "\n",
      "\n",
      " for best model :\n",
      " Epoch: 59\tTrain Loss: tensor(0.6699, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\tVal AUC: 0.8400272764730653\n",
      "time taken for training :  568.3860099315643\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2309ad44216d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtimetaken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time taken for training : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimetaken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time taken for per epoch : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimetaken\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#test_auc = eval_link_predictor(best_model, test_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#print(f\"Test: {test_auc:.3f}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2309ad44216d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtimetaken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time taken for training : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimetaken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time taken for per epoch : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimetaken\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#test_auc = eval_link_predictor(best_model, test_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#print(f\"Test: {test_auc:.3f}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Net(3, 3, 2) #.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "import time\n",
    "curr = time.time()\n",
    "best_model,val_auc_list,train_loss_list = train_link_predictor(model, train_data, \n",
    "                                                                              val_data, optimizer, criterion,n_epochs=250)\n",
    "timetaken = time.time()-curr\n",
    "print(\"time taken for training : \",timetaken)\n",
    "print(\"time taken for per epoch : \",timetaken/epochs)\n",
    "#test_auc = eval_link_predictor(best_model, test_data)\n",
    "#print(f\"Test: {test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c0e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"gat_1_params\")\n",
    "torch.save(model, \"gat_1_for_whole_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08713c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
